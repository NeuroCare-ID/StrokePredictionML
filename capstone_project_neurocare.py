# -*- coding: utf-8 -*-
"""Capstone Project NeuroCare

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11rKA5uBwrGYVPXCxsv_yvuAf-Pb8dRcu

# Capstone Project

## Import Semua Packages/Library yang Digunakan
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

"""## Data Preparation

### Data Loading
"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d fedesoriano/stroke-prediction-dataset
!unzip stroke-prediction-dataset.zip

df = pd.read_csv('healthcare-dataset-stroke-data.csv')
df.drop(['id', 'work_type'], axis=1, inplace=True)
df

"""### Data Cleaning"""

df.info()

df.describe()

cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()

for col in cat_cols:
    print(f"\n=== Fitur: {col} ===")
    vc = df[col].value_counts(dropna=False)
    pct = df[col].value_counts(normalize=True, dropna=False).mul(100).round(2)
    summary = pd.DataFrame({
        'count': vc,
        'percent (%)': pct
    })
    print(summary)

df = df[df['gender'] != 'Other'].copy()

"""#### Handling Missing Value"""

df.isna().sum()

from sklearn.impute import SimpleImputer

imp_median = SimpleImputer(strategy='median')
df[['bmi']] = imp_median.fit_transform(df[['bmi']])

"""#### Handling Imbalance Dataset"""

from collections import Counter
from imblearn.over_sampling import SMOTENC

X = df.drop(columns='stroke')
y = df['stroke']

cat_cols = X.select_dtypes(include=['object']).columns.tolist()
cat_indices = [X.columns.get_loc(col) for col in cat_cols]

print("Kolom kategorikal:", cat_cols)
print("Indeks kategorikal:", cat_indices)

smote_nc = SMOTENC(
    categorical_features=cat_indices,
    sampling_strategy=1.0,
    random_state=42
)
X_sm, y_sm = smote_nc.fit_resample(X, y)

print("Distribusi sebelum SMOTENC:", Counter(y))
print("Distribusi setelah SMOTENC:", Counter(y_sm))

df = pd.DataFrame(X_sm, columns=X.columns)

df['stroke'] = y_sm
df = df.reset_index(drop=True)

print("\nShape df:", df.shape)
display(df.head())

"""### Exploratory Data Analysis

#### Univariate Analysis
"""

numerical_features = df.select_dtypes(include='number').columns
categorical_features = df.select_dtypes(include='object').columns.to_list()

for feature in categorical_features:
    count = df[feature].value_counts()
    percent = 100 * df[feature].value_counts(normalize=True)

    summary_df = pd.DataFrame({
        'jumlah sampel': count,
        'persentase': percent.round(1)
    })
    print(f"\nUnivariate summary for '{feature}':")
    print(summary_df)

    count.plot(kind='bar', title=feature)
    plt.xlabel(feature)
    plt.ylabel("Count")
    plt.show()

df.hist(bins=50, figsize=(20,15))
plt.show()

"""#### Multivariate Analysis"""

for col in categorical_features:
  sns.catplot(x=col, y="stroke", kind="bar", dodge=False, height = 4, aspect = 3,  data=df, palette="Set3")
  plt.title("Rata-rata 'stroke' Relatif terhadap - {}".format(col))

sns.pairplot(df, diag_kind = 'kde')

plt.figure(figsize=(10, 8))
correlation_matrix = df[numerical_features].corr().round(2)

sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""## Data Pre-Processing"""

df_encoded = pd.get_dummies(df, columns=categorical_features)
df_encoded

X = df_encoded.drop(columns='stroke')
y = df_encoded['stroke']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f'Total sample semua dataset: {len(X)}')
print(f'Total sample train dataset: {len(X_train)}')
print(f'Total sample test dataset: {len(X_test)}')

from sklearn.preprocessing import MinMaxScaler
# Standarisasi
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""## Modelling"""

from tensorflow.keras.callbacks import EarlyStopping, Callback

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        if logs.get('accuracy') > 0.9:
            print("\nAkurasi telah mencapai > 90%!")
            self.model.stop_training = True

callbacks = myCallback()

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization

model = Sequential([
    Dense(128, input_dim=X_train.shape[1], activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    Dense(64, activation='relu'),
    BatchNormalization(),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.003),
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.summary()

history = model.fit(
    X_train, y_train,
    epochs=200,
    batch_size=128,
    validation_split=0.2,
    callbacks=[callbacks],
    verbose=2
)